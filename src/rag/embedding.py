import os
import json
import glob
from typing import List, Dict, Any, Tuple
from pathlib import Path
from langchain_pinecone import PineconeEmbeddings
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
import time
import pickle
import threading
from datetime import datetime, timedelta

load_dotenv(verbose=True)

class DataEmbedder:
    def __init__(self, 
                 pinecone_api_key: str,
                 embedding_model: str = "multilingual-e5-large",
                 index_name: str = "openbadges"):
        """
        ë°ì´í„° ì„ë² ë”©ì„ ìœ„í•œ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            pinecone_api_key: Pinecone API í‚¤
            embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            index_name: Pinecone ì¸ë±ìŠ¤ ì´ë¦„
        """
        self.embedding_model = embedding_model
        self.index_name = index_name
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = PineconeEmbeddings(
            model=embedding_model,
            pinecone_api_key=pinecone_api_key
        )
        
        # Pinecone ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=pinecone_api_key)
        self._initialize_index()
        
        # ì‚­ì œëœ ë²¡í„° ë°±ì—…ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self._deleted_vectors_backup = {}
        
    def _initialize_index(self):
        """Pinecone ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        cloud = os.environ.get('PINECONE_CLOUD', 'aws')
        region = os.environ.get('PINECONE_REGION', 'us-east-1')
        spec = ServerlessSpec(cloud=cloud, region=region)
        
        if self.index_name not in self.pc.list_indexes().names():
            self.pc.create_index(
                name=self.index_name,
                dimension=self.embeddings.dimension,
                metric="cosine",
                spec=spec
            )
            # ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            while not self.pc.describe_index(self.index_name).status['ready']:
                time.sleep(1)
    
    def _determine_namespace_from_id(self, vector_id: str) -> str:
        """
        ë²¡í„° IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²°ì •
        
        Args:
            vector_id: ë²¡í„° ID
            
        Returns:
            ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ('badge' ë˜ëŠ” 'user')
            
        Raises:
            ValueError: ID í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
        """
        if not vector_id:
            raise ValueError("ë²¡í„° IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
        if vector_id.upper().startswith('B'):
            return 'badge'
        elif vector_id.upper().startswith('U'):
            return 'user'
        else:
            raise ValueError(
                f"ì˜¬ë°”ë¥´ì§€ ì•Šì€ ID í˜•ì‹ì…ë‹ˆë‹¤. 'B' ë˜ëŠ” 'U'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤: "
                f"{vector_id}"
            )
    
    def _backup_vector_to_file(self, vector_id: str, namespace: str, 
                               vector_data: Dict[str, Any]):
        """
        ì‚­ì œë  ë²¡í„°ë¥¼ íŒŒì¼ë¡œ ë°±ì—… (30ë¶„ í›„ ìë™ ì‚­ì œ)
        
        Args:
            vector_id: ë²¡í„° ID
            namespace: ë„¤ì„ìŠ¤í˜ì´ìŠ¤
            vector_data: ë²¡í„° ë°ì´í„° (id, values, metadata í¬í•¨)
        """
        backup_dir = "backup/deleted_vectors"
        os.makedirs(backup_dir, exist_ok=True)
        
        backup_data = {
            'vector_id': vector_id,
            'namespace': namespace,
            'vector_data': vector_data,
            'deleted_at': datetime.now().isoformat(),
            'expires_at': (datetime.now() + timedelta(minutes=30)).isoformat()
        }
        
        backup_file = os.path.join(backup_dir, f"{vector_id}.pkl")
        
        try:
            with open(backup_file, 'wb') as f:
                pickle.dump(backup_data, f)
            
            # 30ë¶„ í›„ ë°±ì—… íŒŒì¼ ìë™ ì‚­ì œë¥¼ ìœ„í•œ íƒ€ì´ë¨¸ ì„¤ì •
            timer = threading.Timer(1800, self._cleanup_backup_file, 
                                  args=[backup_file])
            timer.daemon = True
            timer.start()
            
            print(f"ğŸ—‚ï¸ ë²¡í„° ë°±ì—… ì™„ë£Œ: {backup_file}")
            
        except Exception as e:
            print(f"âš ï¸ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
    
    def _backup_vector_to_memory(self, vector_id: str, namespace: str, 
                                 vector_data: Dict[str, Any]):
        """
        ì‚­ì œë  ë²¡í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë°±ì—… (30ë¶„ í›„ ìë™ ì‚­ì œ)
        
        Args:
            vector_id: ë²¡í„° ID
            namespace: ë„¤ì„ìŠ¤í˜ì´ìŠ¤
            vector_data: ë²¡í„° ë°ì´í„°
        """
        backup_data = {
            'vector_id': vector_id,
            'namespace': namespace,
            'vector_data': vector_data,
            'deleted_at': datetime.now(),
            'expires_at': datetime.now() + timedelta(minutes=30)
        }
        
        self._deleted_vectors_backup[vector_id] = backup_data
        
        # 30ë¶„ í›„ ë©”ëª¨ë¦¬ì—ì„œ ìë™ ì‚­ì œ
        timer = threading.Timer(1800, self._cleanup_memory_backup, 
                                  args=[vector_id])
        timer.daemon = True
        timer.start()
        
        print(f"ğŸ’¾ ë²¡í„° ë©”ëª¨ë¦¬ ë°±ì—… ì™„ë£Œ: {vector_id}")
    
    def _cleanup_backup_file(self, backup_file: str):
        """ë°±ì—… íŒŒì¼ ìë™ ì‚­ì œ"""
        try:
            if os.path.exists(backup_file):
                os.remove(backup_file)
                print(f"ğŸ—‘ï¸ ë°±ì—… íŒŒì¼ ìë™ ì‚­ì œ: {backup_file}")
        except Exception as e:
            print(f"âš ï¸ ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    
    def _cleanup_memory_backup(self, vector_id: str):
        """ë©”ëª¨ë¦¬ ë°±ì—… ìë™ ì‚­ì œ"""
        try:
            if vector_id in self._deleted_vectors_backup:
                del self._deleted_vectors_backup[vector_id]
                print(f"ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ë°±ì—… ìë™ ì‚­ì œ: {vector_id}")
        except Exception as e:
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    
    def restore_vector(self, vector_id: str) -> Tuple[bool, str]:
        """
        ë°±ì—…ëœ ë²¡í„°ë¥¼ ë³µì›
        
        Args:
            vector_id: ë³µì›í•  ë²¡í„° ID
            
        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            # ë©”ëª¨ë¦¬ ë°±ì—…ì—ì„œ í™•ì¸
            if vector_id in self._deleted_vectors_backup:
                backup_data = self._deleted_vectors_backup[vector_id]
                
                if datetime.now() <= backup_data['expires_at']:
                    # ë²¡í„° ë³µì›
                    index = self.pc.Index(self.index_name)
                    vector_data = backup_data['vector_data']
                    
                    index.upsert(
                        vectors=[(
                            vector_data['id'],
                            vector_data['values'],
                            vector_data['metadata']
                        )],
                        namespace=backup_data['namespace']
                    )
                    
                    # ë°±ì—…ì—ì„œ ì œê±°
                    del self._deleted_vectors_backup[vector_id]
                    
                    return True, f"âœ… ë²¡í„° ë³µì› ì„±ê³µ: {vector_id}"
                else:
                    del self._deleted_vectors_backup[vector_id]
                    return False, f"âŒ ë³µì› ê¸°ê°„ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {vector_id}"
            
            # íŒŒì¼ ë°±ì—…ì—ì„œ í™•ì¸
            backup_file = f"backup/deleted_vectors/{vector_id}.pkl"
            if os.path.exists(backup_file):
                with open(backup_file, 'rb') as f:
                    backup_data = pickle.load(f)
                
                expires_at = datetime.fromisoformat(backup_data['expires_at'])
                if datetime.now() <= expires_at:
                    # ë²¡í„° ë³µì›
                    index = self.pc.Index(self.index_name)
                    vector_data = backup_data['vector_data']
                    
                    index.upsert(
                        vectors=[(
                            vector_data['id'],
                            vector_data['values'],
                            vector_data['metadata']
                        )],
                        namespace=backup_data['namespace']
                    )
                    
                    # ë°±ì—… íŒŒì¼ ì‚­ì œ
                    os.remove(backup_file)
                    
                    return True, f"âœ… ë²¡í„° ë³µì› ì„±ê³µ: {vector_id}"
                else:
                    os.remove(backup_file)
                    return False, f"âŒ ë³µì› ê¸°ê°„ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {vector_id}"
            
            return False, f"âŒ ë°±ì—…ëœ ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vector_id}"
            
        except Exception as e:
            return False, f"âŒ ë²¡í„° ë³µì› ì‹¤íŒ¨: {str(e)}"
    
    def delete_vector(self, vector_id: str, backup_method: str = "memory") -> Tuple[bool, str]:
        """
        Pineconeì—ì„œ ë²¡í„°ë¥¼ ì‚­ì œ (30ë¶„ê°„ ë³µì› ê°€ëŠ¥í•˜ë„ë¡ ë°±ì—…)
        
        Args:
            vector_id: ì‚­ì œí•  ë²¡í„° ID (Bë¡œ ì‹œì‘í•˜ë©´ badge, Uë¡œ ì‹œì‘í•˜ë©´ user)
            backup_method: ë°±ì—… ë°©ë²• ("memory" ë˜ëŠ” "file")
            
        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            # 1. IDë¡œë¶€í„° ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²°ì •
            namespace = self._determine_namespace_from_id(vector_id)
            data_type = "ë°°ì§€" if namespace == "badge" else "ì‚¬ìš©ì"
            
            # 2. ë²¡í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            index = self.pc.Index(self.index_name)
            try:
                existing_vector = index.fetch(ids=[vector_id], namespace=namespace)
                if (not existing_vector.vectors or 
                    vector_id not in existing_vector.vectors):
                    return False, (f"âŒ í•´ë‹¹ {data_type}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: "
                                  f"{vector_id}")
                
                # 3. ì‚­ì œ ì „ ë°±ì—…
                vector_data = existing_vector.vectors[vector_id]
                backup_data = {
                    'id': vector_id,
                    'values': vector_data.values,
                    'metadata': vector_data.metadata
                }
                
                if backup_method == "file":
                    self._backup_vector_to_file(vector_id, namespace, 
                                              backup_data)
                else:
                    self._backup_vector_to_memory(vector_id, namespace, 
                                                backup_data)
                
            except Exception as e:
                return False, f"âŒ ë²¡í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            # 4. ë²¡í„° ì‚­ì œ
            try:
                index.delete(ids=[vector_id], namespace=namespace)
                return True, (f"âœ… {data_type} ì‚­ì œ ì„±ê³µ: {vector_id} "
                              f"(30ë¶„ê°„ ë³µì› ê°€ëŠ¥)")
                
            except Exception as e:
                return False, f"âŒ {data_type} ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
                
        except ValueError as e:
            return False, f"âŒ {str(e)}"
        except Exception as e:
            return False, f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"

    def _determine_data_type(self, file_path: str) -> str:
        """
        JSON íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë°ì´í„° íƒ€ì…(badge ë˜ëŠ” user)ì„ ê²°ì •
        
        Args:
            file_path: JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë°ì´í„° íƒ€ì… ('badge' ë˜ëŠ” 'user')
            
        Raises:
            ValueError: ë°ì´í„° íƒ€ì…ì„ ê²°ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš°
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # badge ë°ì´í„°ì˜ íŠ¹ì§•ì ì¸ í•„ë“œë“¤ í™•ì¸
            badge_fields = {
                'badge_id', 'name', 'issuer', 'criteria', 'skillsValidated'
            }
            # user ë°ì´í„°ì˜ íŠ¹ì§•ì ì¸ í•„ë“œë“¤ í™•ì¸
            user_fields = {
                'user_id', 'goal', 'competency_level', 'learning_history'
            }
            
            data_keys = set(data.keys())
            
            # badge í•„ë“œì™€ì˜ êµì§‘í•©ì´ ë” ë§ìœ¼ë©´ badge
            badge_match = len(badge_fields.intersection(data_keys))
            user_match = len(user_fields.intersection(data_keys))
            
            if badge_match > user_match:
                return 'badge'
            elif user_match > badge_match:
                return 'user'
            else:
                # íŒŒì¼ëª…ìœ¼ë¡œë„ í™•ì¸
                filename = os.path.basename(file_path).lower()
                if 'badge' in filename:
                    return 'badge'
                elif 'user' in filename:
                    return 'user'
                else:
                    raise ValueError(
                        f"ë°ì´í„° íƒ€ì…ì„ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                    
        except Exception as e:
            raise ValueError(
                f"íŒŒì¼ì„ ì½ê±°ë‚˜ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {file_path}, "
                f"ì˜¤ë¥˜: {str(e)}")
    
    def upsert_vector(self, file_path: str):
        """
        íŠ¹ì • JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Pineconeì— ë²¡í„°ë¥¼ upsert
        
        Args:
            file_path: ì²˜ë¦¬í•  JSON íŒŒì¼ ê²½ë¡œ
        """
        try:
            # 1. íŒŒì¼ íƒ€ì… í™•ì¸
            data_type = self._determine_data_type(file_path)
            print(f"íŒŒì¼ íƒ€ì… í™•ì¸: {data_type}")
            
            # 2. íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if data_type == 'badge':
                processed_item = self.preprocess_badge(data)
            else:
                processed_item = self.preprocess_user(data)
            
            # 3. ë™ì¼í•œ IDê°€ ìˆëŠ”ì§€ í™•ì¸
            index = self.pc.Index(self.index_name)
            vector_id = processed_item["id"]
            
            try:
                existing_vector = index.fetch(
                    ids=[vector_id], namespace=data_type
                )
                if existing_vector.vectors:
                    print(f"ë²¡í„° ì—…ë°ì´íŠ¸: ID {vector_id} "
                          f"(ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {data_type})")
                else:
                    print(f"ìƒˆ ë²¡í„° ì‚½ì…: ID {vector_id} "
                          f"(ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {data_type})")
            except Exception:
                print(f"ìƒˆ ë²¡í„° ì‚½ì…: ID {vector_id} "
                      f"(ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {data_type})")
            
            # 4. ë²¡í„° ì„ë² ë”© ìƒì„±
            vector = self.embeddings.embed_query(processed_item["text"])
            
            # 5. Pineconeì— upsert
            try:
                index.upsert(
                    vectors=[(vector_id, vector, processed_item["metadata"])],
                    namespace=data_type
                )
                
                # ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥
                name = processed_item["metadata"].get("name", "ì´ë¦„ ì—†ìŒ")
                print(f"âœ… upsert ì„±ê³µ: {name} (ID: {vector_id}, "
                      f"íƒ€ì…: {data_type})")
                
            except Exception as e:
                print(f"âŒ upsert ì‹¤íŒ¨: ID {vector_id}, ì˜¤ë¥˜: {str(e)}")
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")

    def preprocess_badge(self, badge_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë°°ì§€ ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            badge_data: ì›ë³¸ ë°°ì§€ JSON ë°ì´í„°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°°ì§€ ë°ì´í„°
        """
        # ë°°ì§€ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        text = f"""
        ë°°ì§€ëª…: {badge_data.get('name', '')}
        ë°œê¸‰ì: {badge_data.get('issuer', '')}
        ì„¤ëª…: {badge_data.get('description', '')}
        ê¸°ì¤€: {badge_data.get('criteria', '')}
        ì •ë ¬: {badge_data.get('alignment', '')}
        ì·¨ì—… ê²°ê³¼: {badge_data.get('employmentOutcome', '')}
        ê²€ì¦ëœ ê¸°ìˆ : {badge_data.get('skillsValidated', '')}
        ì—­ëŸ‰: {badge_data.get('competency', '')}
        í•™ìŠµ ê¸°íšŒ: {badge_data.get('learningOpportunity', '')}
        """
        
        return {
            "id": badge_data.get('badge_id'),
            "text": text,
            "metadata": {
                "name": badge_data.get('name'),
                "issuer": badge_data.get('issuer'),
                "skills": badge_data.get('skillsValidated'),
                "competency": badge_data.get('competency'),
                "related_badges": badge_data.get('related_badges')
            }
        }
    
    def preprocess_user(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            user_data: ì›ë³¸ ì‚¬ìš©ì JSON ë°ì´í„°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ì‚¬ìš©ì ë°ì´í„°
        """
        # ì‚¬ìš©ì ì •ë³´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        text = f"""
        ì´ë¦„: {user_data.get('name', '')}
        ëª©í‘œ: {user_data.get('goal', '')}
        ê¸°ìˆ : {user_data.get('skills', '')}
        ì—­ëŸ‰ ìˆ˜ì¤€: {user_data.get('competency_level', '')}
        í•™ìŠµ ì´ë ¥: {user_data.get('learning_history', '')}
        ì·¨ì—… ì´ë ¥: {user_data.get('employment_history', '')}
        êµìœ¡ ìˆ˜ì¤€: {user_data.get('education_level', '')}
        ì°¸ì—¬ ì§€í‘œ: {user_data.get('engagement_metrics', '')}
        """
        
        return {
            "id": user_data.get('user_id'),
            "text": text,
            "metadata": {
                "name": user_data.get('name'),
                "goal": user_data.get('goal'),
                "skills": user_data.get('skills'),
                "competency_level": user_data.get('competency_level'),
                "acquired_badges": user_data.get('acquired_badges'),
                "education_level": user_data.get('education_level')
            }
        }
    
    def upsert_all(self, data_dir: str, data_type: str):
        """
        ë°ì´í„° íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  Pineconeì— ì €ì¥ (ê¸°ì¡´ process_dataì—ì„œ ì´ë¦„ ë³€ê²½)
        
        Args:
            data_dir: ë°ì´í„° JSON íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
            data_type: ë°ì´í„° íƒ€ì… ('badge' ë˜ëŠ” 'user')
        """
        if data_type not in ['badge', 'user']:
            raise ValueError("data_typeì€ 'badge' ë˜ëŠ” 'user'ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ íŒ¨í„´ ì„¤ì •
        file_pattern = f"{data_type}_*.json"
        data_files = glob.glob(os.path.join(data_dir, file_pattern))
        processed_data = []
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        for data_file in data_files:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data_type == 'badge':
                    processed_item = self.preprocess_badge(data)
                else:
                    processed_item = self.preprocess_user(data)
                processed_data.append(processed_item)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë° Pineconeì— ì €ì¥
        texts = [item["text"] for item in processed_data]
        metadatas = [item["metadata"] for item in processed_data]
        ids = [item["id"] for item in processed_data]
        
        # Pineconeì— ë°ì´í„° ì €ì¥ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì§€ì •)
        index = self.pc.Index(self.index_name)
        for text, metadata, id in zip(texts, metadatas, ids):
            vector = self.embeddings.embed_query(text)
            index.upsert(
                vectors=[(id, vector, metadata)],
                namespace=data_type
            )
        
        print(f"{data_type} ë°ì´í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì™„ë£Œ: {len(texts)}ê°œ")

def main():
    """
    /data ì˜ json ë°°ì§€/ìœ ì € íŒŒì¼ ì„ë² ë”©
    
    """
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    pinecone_api_key = os.environ.get("PINECONE_API_KEY")
    if not pinecone_api_key:
        raise ValueError("PINECONE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    data_dir = "data/json"
    
    # ì„ë² ë” ì´ˆê¸°í™”
    embedder = DataEmbedder(pinecone_api_key=pinecone_api_key)
    
    # ë°°ì§€ ë°ì´í„° ì²˜ë¦¬
    embedder.upsert_all(os.path.join(data_dir, "badge"), "badge")
    
    # ì‚¬ìš©ì ë°ì´í„° ì²˜ë¦¬
    embedder.upsert_all(os.path.join(data_dir, "user"), "user")

if __name__ == "__main__":
    main()